{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When working in AI LRZ\n",
    "%cd ~/cma/CMA_Fairness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run analysis_setup_cp.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "RUN_TO_ANALYSE = \"1\"\n",
    "OUTPUT_DIR = Path(\".\") / \"output\"\n",
    "\n",
    "# Directory that will contain outputs from analysis\n",
    "ANALYSIS_OUTPUT_DIR = OUTPUT_DIR / \"analyses\" / (str(RUN_TO_ANALYSE))\n",
    "\n",
    "DATA_DIR = OUTPUT_DIR / \"runs\" / str(RUN_TO_ANALYSE) / \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREFIX_SETTINGS = \"sett_\"\n",
    "PREFIX_EVAL = \"sett_eval_\"\n",
    "PREFIX_PERFORMANCE = \"perf_\"\n",
    "PREFIX_FAIRNESS = \"fair_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_fairness_metric = \"fair_main_equalized_odds_difference\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg_raw = pd.read_csv(DATA_DIR / f\"agg_{RUN_TO_ANALYSE}_run_outputs.csv.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "df_settings = pd.json_normalize(\n",
    "    df_agg_raw[\"universe_settings\"].apply(json.loads)\n",
    ").add_prefix(\n",
    "    PREFIX_SETTINGS\n",
    ")\n",
    "\n",
    "df_agg_full = df_settings.join(df_agg_raw)\n",
    "df_agg_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows, columns = df_agg_full.shape\n",
    "print(f\"The data has N = {rows} rows and N = {columns} columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_settings = list(df_agg_full.columns[df_agg_full.columns.str.startswith(PREFIX_SETTINGS)])\n",
    "cols_eval = list(df_agg_full.columns[df_agg_full.columns.str.startswith(PREFIX_EVAL)])\n",
    "cols_non_eval = list(set(cols_settings) - set(cols_eval))\n",
    "cols_performance = list(df_agg_full.columns[df_agg_full.columns.str.startswith(PREFIX_PERFORMANCE)])\n",
    "cols_fairness = list(df_agg_full.columns[df_agg_full.columns.str.startswith(PREFIX_FAIRNESS)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg_full[\"universe_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_mask = df_agg_full[\"sett_exclude_subgroups\"].isin(['keep-largest_race_1'])\n",
    "print(f\"Dropping N = {drop_mask.sum()} rows, keeping N = {(~drop_mask).sum()}\")\n",
    "\n",
    "df_agg_full = df_agg_full[~drop_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg_full[\"sett_exclude_subgroups\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_agg_full[\"sett_eval_fairness_grouping\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Parsed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg_full.to_csv(ANALYSIS_OUTPUT_DIR / \"df_agg_full.csv.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Data by Fairness Grouping\n",
    "\n",
    "Most analyses only make sense for one value of fairness grouping, so we explicitly filter the data here to use one of the two values and create a new dataframe that holds the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg = df_agg_full[df_agg_full[\"sett_eval_fairness_grouping\"] == \"majority-minority\"]\n",
    "rows, columns = df_agg.shape\n",
    "print(f\"The data has N = {rows} rows and N = {columns} columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.histogram(df_agg_full, x=\"fair_main_equalized_odds_difference\", marginal=\"rug\")\n",
    "fig.show(renderer=\"notebook\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.histogram(df_agg_full, x=\"fair_main_equalized_odds_difference\", color=\"sett_eval_fairness_grouping\", marginal=\"rug\")\n",
    "fig.show(renderer=\"notebook\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg_full[\"sett_eval_fairness_grouping\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriptive Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Helper Function to get interactive refreshing dropdowns\n",
    "def interactive_single_var_dropdown(options, render_function, description='Column:'):\n",
    "    dd = widgets.Dropdown(\n",
    "        options=options,\n",
    "        description=description,\n",
    "    )\n",
    "\n",
    "    def refresh():\n",
    "        display(dd)\n",
    "\n",
    "        render_function(dd.value)\n",
    "\n",
    "    def on_change(change):\n",
    "        if change['type'] == 'change' and change['name'] == 'value':\n",
    "            clear_output()\n",
    "\n",
    "            refresh()\n",
    "\n",
    "    dd.observe(on_change)\n",
    "\n",
    "    refresh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_simple_density(colname):\n",
    "    # Show default density plot\n",
    "    df_agg[colname].plot.kde()\n",
    "\n",
    "interactive_single_var_dropdown(options = cols_fairness + cols_performance, render_function=render_simple_density)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "def render_plotly_density(colname):\n",
    "    fig = px.histogram(\n",
    "        df_agg,\n",
    "        x=colname,\n",
    "        marginal=\"rug\",\n",
    "        hover_data=cols_settings\n",
    "    )\n",
    "    fig.show(renderer=\"notebook\")\n",
    "\n",
    "interactive_single_var_dropdown(options = cols_fairness + cols_performance, render_function=render_plotly_density)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(\n",
    "    df_agg,\n",
    "    x=\"perf_ovrl_accuracy\",\n",
    "    y=main_fairness_metric,\n",
    "    marginal_x=\"violin\",\n",
    "    marginal_y= \"violin\",\n",
    "    hover_data=cols_settings,\n",
    "    title=\"Accuracy x Fairness\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Analysis of Fairness based on Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "def fairness_violin(column_to_compare):\n",
    "    fig = px.violin(\n",
    "        df_agg,\n",
    "        x = column_to_compare,\n",
    "        y = main_fairness_metric,\n",
    "        color = column_to_compare,\n",
    "        points = \"all\",\n",
    "        hover_data = cols_settings\n",
    "    )\n",
    "    # fig.update_traces(pointpos=0)\n",
    "    display(fig)\n",
    "\n",
    "interactive_single_var_dropdown(options = cols_settings, render_function=fairness_violin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (CMA Fairness)",
   "language": "python",
   "name": "cma_fair_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
