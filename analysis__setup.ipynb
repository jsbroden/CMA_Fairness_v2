{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/dss/dsshome1/0C/ra93lal2/cma/CMA_Fairness_v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dss/dsshome1/0C/ra93lal2/.local/share/virtualenvs/CMA_Fairness_v2-3j10GkSs/lib/python3.10/site-packages/IPython/core/magics/osm.py:393: UserWarning: This is now an optional IPython functionality, using bookmarks requires you to install the `pickleshare` library.\n",
      "  bkms = self.shell.db.get('bookmarks', {})\n",
      "/dss/dsshome1/0C/ra93lal2/.local/share/virtualenvs/CMA_Fairness_v2-3j10GkSs/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "# When working in AI LRZ\n",
    "%cd ~/cma/CMA_Fairness_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-reload the custom package\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport fairness_multiverse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_TO_ANALYSE = \"8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREFIX_SETTINGS = \"sett_\"\n",
    "PREFIX_EVAL = \"sett_eval_\"\n",
    "PREFIX_PERFORMANCE = \"perf_\"\n",
    "PREFIX_FAIRNESS = \"fair_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "OUTPUT_DIR = Path(\".\") / \"output\"\n",
    "\n",
    "# Directory that will contain outputs from analysis\n",
    "ANALYSIS_OUTPUT_DIR = OUTPUT_DIR / \"analyses\" / (str(RUN_TO_ANALYSE))\n",
    "ANALYSIS_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DATA_DIR = OUTPUT_DIR / \"runs\" / str(RUN_TO_ANALYSE) / \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_fairness_metric = \"fair_main_equalized_odds_difference\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg_raw = pd.read_csv(DATA_DIR / f\"agg_{RUN_TO_ANALYSE}_run_outputs.csv.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "df_settings = pd.json_normalize(\n",
    "    df_agg_raw[\"universe_settings\"].apply(json.loads)\n",
    ").add_prefix(\n",
    "    PREFIX_SETTINGS\n",
    ")\n",
    "\n",
    "df_agg_full = df_settings.join(df_agg_raw)\n",
    "df_agg_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows, columns = df_agg_full.shape\n",
    "print(f\"The data has N = {rows} rows and N = {columns} columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_settings = list(df_agg_full.columns[df_agg_full.columns.str.startswith(PREFIX_SETTINGS)])\n",
    "cols_eval = list(df_agg_full.columns[df_agg_full.columns.str.startswith(PREFIX_EVAL)])\n",
    "cols_non_eval = list(set(cols_settings) - set(cols_eval))\n",
    "cols_performance = list(df_agg_full.columns[df_agg_full.columns.str.startswith(PREFIX_PERFORMANCE)])\n",
    "cols_fairness = list(df_agg_full.columns[df_agg_full.columns.str.startswith(PREFIX_FAIRNESS)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg_full[\"universe_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_mask = df_agg_full[\"sett_exclude_subgroups\"].isin(['drop-non-german'])\n",
    "print(f\"Dropping N = {drop_mask.sum()} rows, keeping N = {(~drop_mask).sum()}\")\n",
    "\n",
    "df_agg_full = df_agg_full[~drop_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg_full[\"sett_exclude_subgroups\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg_full[\"sett_eval_fairness_grouping\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_agg_full[\"sett_eval_exclude_subgroups\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_agg_full[\"sett_eval_on_subset\"].unique()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (CMA Fairness)",
   "language": "python",
   "name": "cma_fair_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
