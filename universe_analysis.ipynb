{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f159f837-637b-42ba-96b5-ec9a68c44524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/dss/dsshome1/0C/ra93lal2/cma/CMA_Fairness_v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dss/dsshome1/0C/ra93lal2/.local/share/virtualenvs/CMA_Fairness_v2-3j10GkSs/lib/python3.10/site-packages/IPython/core/magics/osm.py:393: UserWarning: This is now an optional IPython functionality, using bookmarks requires you to install the `pickleshare` library.\n",
      "  bkms = self.shell.db.get('bookmarks', {})\n",
      "/dss/dsshome1/0C/ra93lal2/.local/share/virtualenvs/CMA_Fairness_v2-3j10GkSs/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd ~/cma/CMA_Fairness_v2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2603b9",
   "metadata": {},
   "source": [
    "The following cell holds the definition of our parameters, these values can be overriden by rendering the with e.g. the following command:\n",
    "\n",
    "papermill -p alpha 0.2 -p ratio 0.3 universe_analysis.ipynb output/test_run.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a80968a0-40bb-4fa9-85ef-2d5eefb01975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /dss/dsshome1/0C/ra93lal2/cma/CMA_Fairness_v2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"Current working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2dce4c03",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "run_no = 0\n",
    "universe_id = \"test\"\n",
    "universe = {\n",
    "    \"scale\": \"scale\", # \"scale\", \"do-not-scale\",\n",
    "    \"stratify_split\": \"target\", # \"none\", \"target\", \"protected-attribute\", \"both\",\n",
    "    \"model\": \"elasticnet\", # \"logreg\", \"penalized_logreg\", \"rf\", \"gbm\", \"elasticnet\"\n",
    "    \"cutoff\": [\"quantile_0.15\", \"quantile_0.30\"],\n",
    "    \"exclude_features\": \"age\", # \"none\", \"nationality\", \"sex\", \"nationality-sex\", \"age\"\n",
    "    \"exclude_subgroups\": \"keep-all\", # \"keep-all\", \"drop-non-german\"\n",
    "    #\"training_sample\": \"restricted\", # \"restricted\", \"full\"\n",
    "    \"eval_fairness_grouping\": [\"majority-minority\", \"nationality-all\"]\n",
    "}\n",
    "\n",
    "output_dir=\"./output\"\n",
    "seed=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1650acaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# Parse universe into dict if it is passed as a string\n",
    "if isinstance(universe, str):\n",
    "    universe = json.loads(universe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16620c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-reload the custom package\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport fairness_multiverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01c5c9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairness_multiverse.universe import UniverseAnalysis\n",
    "\n",
    "universe_analysis = UniverseAnalysis(\n",
    "    run_no = run_no,\n",
    "    universe_id = universe_id,\n",
    "    universe = universe,\n",
    "    output_dir=output_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "106241f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Seed: 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "parsed_seed = int(seed)\n",
    "np.random.seed(parsed_seed)\n",
    "print(f\"Using Seed: {parsed_seed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ebdc57",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681925a3",
   "metadata": {},
   "source": [
    "Load siab_train, siab_test, siab_calib and/or \n",
    "load siab_train_features, siab_train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0496b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SIAB data from cache: data/siab_cached.csv.gz\n",
      "(643690, 164)\n"
     ]
    }
   ],
   "source": [
    "# Do I need to load siab? Delete this cell?\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# File paths\n",
    "raw_file = Path(\"data/raw/siab.csv\")\n",
    "cache_file = Path(\"data/siab_cached.csv.gz\")\n",
    "\n",
    "# Ensure cache directory exists\n",
    "cache_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load with simple caching\n",
    "if cache_file.exists():\n",
    "    print(f\"Loading SIAB data from cache: {cache_file}\")\n",
    "    siab = pd.read_csv(cache_file, compression='gzip')\n",
    "else:\n",
    "    print(f\"Cache not found. Reading raw SIAB data: {raw_file}\")\n",
    "    siab = pd.read_csv(raw_file)\n",
    "    siab.to_csv(cache_file, index=False, compression='gzip')\n",
    "    print(f\"Cached SIAB data to: {cache_file}\")\n",
    "\n",
    "# Now use `siab` DataFrame as needed\n",
    "print(siab.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db0ca512-5f53-4dba-abdb-a2888bca41ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>persnr</th>\n",
       "      <th>year</th>\n",
       "      <th>nrEntry</th>\n",
       "      <th>ltue</th>\n",
       "      <th>employed_before</th>\n",
       "      <th>receipt_leh_before</th>\n",
       "      <th>receipt_lhg_before</th>\n",
       "      <th>se_before</th>\n",
       "      <th>ASU_notue_seeking_before</th>\n",
       "      <th>ASU_other_before</th>\n",
       "      <th>...</th>\n",
       "      <th>minijob_tot_dur_byage</th>\n",
       "      <th>ft_tot_dur_byage</th>\n",
       "      <th>befrist_tot_dur_byage</th>\n",
       "      <th>leih_tot_dur_byage</th>\n",
       "      <th>LHG_tot_dur_byage</th>\n",
       "      <th>LEH_tot_dur_byage</th>\n",
       "      <th>almp_tot_dur_byage</th>\n",
       "      <th>almp_aw_tot_dur_byage</th>\n",
       "      <th>se_tot_dur_byage</th>\n",
       "      <th>seeking1_tot_dur_byage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.043478</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18</td>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18</td>\n",
       "      <td>2011</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.714286</td>\n",
       "      <td>2.714286</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.775510</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.367347</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.836735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18</td>\n",
       "      <td>2012</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.200000</td>\n",
       "      <td>4.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.960000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18</td>\n",
       "      <td>2012</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.460000</td>\n",
       "      <td>5.460000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.360000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.320000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.280000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>643685</th>\n",
       "      <td>1827860</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.612903</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>643686</th>\n",
       "      <td>1827860</td>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.212121</td>\n",
       "      <td>0.212121</td>\n",
       "      <td>17.363636</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.909091</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.121212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>643687</th>\n",
       "      <td>1827860</td>\n",
       "      <td>2016</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.205882</td>\n",
       "      <td>34.705882</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.352941</td>\n",
       "      <td>1.705882</td>\n",
       "      <td>1.705882</td>\n",
       "      <td>23.911765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>643688</th>\n",
       "      <td>1827869</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.525424</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>643689</th>\n",
       "      <td>1827869</td>\n",
       "      <td>2014</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.950000</td>\n",
       "      <td>1.033333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>8.133333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>643690 rows × 164 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         persnr  year  nrEntry  ltue  employed_before  receipt_leh_before  \\\n",
       "0             7  2015        1     0                1                   0   \n",
       "1            18  2010        1     1                0                   0   \n",
       "2            18  2011        2     0                1                   0   \n",
       "3            18  2012        3     0                1                   0   \n",
       "4            18  2012        4     0                1                   0   \n",
       "...         ...   ...      ...   ...              ...                 ...   \n",
       "643685  1827860  2013        1     0                0                   0   \n",
       "643686  1827860  2015        2     1                0                   0   \n",
       "643687  1827860  2016        3     1                0                   0   \n",
       "643688  1827869  2013        1     1                1                   0   \n",
       "643689  1827869  2014        2     0                0                   1   \n",
       "\n",
       "        receipt_lhg_before  se_before  ASU_notue_seeking_before  \\\n",
       "0                        0          0                         1   \n",
       "1                        0          0                         0   \n",
       "2                        1          0                         1   \n",
       "3                        1          0                         1   \n",
       "4                        1          0                         1   \n",
       "...                    ...        ...                       ...   \n",
       "643685                   1          0                         0   \n",
       "643686                   1          0                         1   \n",
       "643687                   1          1                         0   \n",
       "643688                   0          0                         1   \n",
       "643689                   0          0                         1   \n",
       "\n",
       "        ASU_other_before  ...  minijob_tot_dur_byage  ft_tot_dur_byage  \\\n",
       "0                      0  ...               0.000000          0.000000   \n",
       "1                      0  ...               0.000000          0.000000   \n",
       "2                      0  ...               2.714286          2.714286   \n",
       "3                      0  ...               4.200000          4.200000   \n",
       "4                      0  ...               5.460000          5.460000   \n",
       "...                  ...  ...                    ...               ...   \n",
       "643685                 1  ...               0.000000          0.000000   \n",
       "643686                 0  ...               0.000000          0.000000   \n",
       "643687                 1  ...               0.000000          0.000000   \n",
       "643688                 0  ...               0.000000          0.000000   \n",
       "643689                 0  ...               0.000000          0.666667   \n",
       "\n",
       "        befrist_tot_dur_byage  leih_tot_dur_byage  LHG_tot_dur_byage  \\\n",
       "0                   15.043478            0.000000           0.000000   \n",
       "1                    0.000000            0.000000           0.000000   \n",
       "2                    0.000000            0.000000          10.775510   \n",
       "3                    0.000000            0.000000          12.100000   \n",
       "4                    0.000000            0.000000          13.360000   \n",
       "...                       ...                 ...                ...   \n",
       "643685               0.000000            0.000000           0.612903   \n",
       "643686               0.212121            0.212121          17.363636   \n",
       "643687               0.294118            0.205882          34.705882   \n",
       "643688               0.000000            0.000000           0.000000   \n",
       "643689               0.666667            0.000000           0.000000   \n",
       "\n",
       "        LEH_tot_dur_byage  almp_tot_dur_byage  almp_aw_tot_dur_byage  \\\n",
       "0                0.000000            0.000000               0.000000   \n",
       "1                0.000000            0.000000               0.000000   \n",
       "2                0.000000            8.367347               0.000000   \n",
       "3                0.000000            9.400000               0.000000   \n",
       "4                0.000000           10.320000               0.000000   \n",
       "...                   ...                 ...                    ...   \n",
       "643685           0.000000            0.000000               0.000000   \n",
       "643686           0.000000            8.909091               0.000000   \n",
       "643687           0.000000           10.352941               1.705882   \n",
       "643688           0.525424            0.000000               0.000000   \n",
       "643689           8.950000            1.033333               0.083333   \n",
       "\n",
       "        se_tot_dur_byage  seeking1_tot_dur_byage  \n",
       "0               0.000000                0.000000  \n",
       "1               0.000000                0.000000  \n",
       "2               0.000000                9.836735  \n",
       "3               0.000000                9.960000  \n",
       "4               0.000000               10.280000  \n",
       "...                  ...                     ...  \n",
       "643685          0.000000                0.000000  \n",
       "643686          0.000000                7.121212  \n",
       "643687          1.705882               23.911765  \n",
       "643688          0.000000                0.000000  \n",
       "643689          0.083333                8.133333  \n",
       "\n",
       "[643690 rows x 164 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "siab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0edb063",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "X_train = pd.read_csv(\"./data/X_train.csv\")\n",
    "y_train = pd.read_csv(\"./data/y_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63d08085",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.read_csv(\"./data/X_test.csv\")\n",
    "y_true = pd.read_csv(\"./data/y_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98a8d0fa-1d3d-4ed8-bb2c-281470e24add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibration data for conformal\n",
    "X_calib = pd.read_csv(\"./data/X_calib.csv\")\n",
    "y_calib = pd.read_csv(\"./data/y_calib.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6c733c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliary data needed downstream in the pipeline\n",
    "\n",
    "org_train = X_train.copy()\n",
    "org_test = X_test.copy()\n",
    "org_calib = X_calib.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4a1b33",
   "metadata": {},
   "source": [
    "# Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca879031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXCLUDE PROTECTED FEATURES\n",
    "# ----------------------\n",
    "# \"exclude_features\": \"none\", # \"nationality\", \"sex\", \"nationality-sex\"\n",
    "\n",
    "excluded_features = universe[\"exclude_features\"].split(\"-\") # split, e.g.: \"nationality-sex\" -> [\"nationality\", \"sex\"]\n",
    "excluded_features_dictionary = {\n",
    "    \"nationality\": [\"maxdeutsch1\", \"maxdeutsch.Missing.\"],\n",
    "    \"sex\": [\"frau1\"],\n",
    "    \"age\": [\"age\"],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b745ac60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code nice names to column names\n",
    "\n",
    "excluded_features_columns = [\n",
    "    excluded_features_dictionary[f] for f in excluded_features if len(f) > 0 and f != \"none\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f84f73a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import flatten_once\n",
    "\n",
    "excluded_features_columns = flatten_once(excluded_features_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "884dea22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping features: ['age']\n"
     ]
    }
   ],
   "source": [
    "if len(excluded_features_columns) > 0:\n",
    "    print(f\"Dropping features: {excluded_features_columns}\")\n",
    "    X_train.drop(excluded_features_columns, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "95ab8b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping features: ['age']\n"
     ]
    }
   ],
   "source": [
    "if len(excluded_features_columns) > 0:\n",
    "    print(f\"Dropping features: {excluded_features_columns}\")\n",
    "    X_test.drop(excluded_features_columns, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1853aaf5-fb9f-46cb-a2b5-8ea4ae2237e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping features: ['age']\n"
     ]
    }
   ],
   "source": [
    "if len(excluded_features_columns) > 0:\n",
    "    print(f\"Dropping features: {excluded_features_columns}\")\n",
    "    X_calib.drop(excluded_features_columns, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6727d079-03e5-481e-9d38-292a90b7a0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPLIT & STRATIFY DATA\n",
    "# ----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15cc6cb-f1c0-4b66-b577-b56901db2185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check thinks that protected group is maxdeutsch1 == 1? That would be wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4b8d88-9d9f-4fa6-b206-1900abe568db",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = siab.groupby('year')\n",
    "siab_s = grouped.apply(lambda x: x.sample(n=5000, random_state=42)).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab456b5-4425-485b-882d-61c7374321eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "siab_train = siab_s[siab_s.year < 2015]\n",
    "siab_calib = siab[siab.year == 2015]\n",
    "siab_test  = siab[siab.year == 2016]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0d81c2-ef89-4ef5-aaad-8d2f4efeb033",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_col = \"ltue\"\n",
    "group_col = \"maxdeutsch1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23f576a-5716-4c21-a7e9-8fa93e026bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = siab_train.iloc[:, 4:164]\n",
    "y_train = siab_train[label_col]\n",
    "group_train = siab_train[group_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16d7780-c334-4f37-b961-6593dce41a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_calib = siab_calib.iloc[:, 4:164]\n",
    "y_calib = siab_calib[label_col]\n",
    "group_calib = siab_calib[group_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c67d40-5930-40a9-9174-d6b69f726abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = siab_test.iloc[:, 4:164]\n",
    "y_test = siab_test[label_col]\n",
    "group_test = siab_test[group_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf030a10-ea34-4d30-828f-4019c2770570",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stratify_col(y, g):\n",
    "    if stratify_split == \"none\":\n",
    "        return None\n",
    "    elif stratify_split == \"target\":\n",
    "        return y\n",
    "    elif stratify_split == \"protected-attribute\":\n",
    "        return g\n",
    "    elif stratify_split == \"both\":\n",
    "        return g.astype(str) + \"-\" + y.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10678f4-52d4-40db-9c99-cdde1c5984ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete later\n",
    "stratify_split = \"both\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77884226-6fb7-44cb-b231-69f22984aa81",
   "metadata": {},
   "outputs": [],
   "source": [
    "stratify_train = get_stratify_col(y_train, group_train)\n",
    "stratify_calib = get_stratify_col(y_calib, group_calib)\n",
    "stratify_test  = get_stratify_col(y_test, group_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e08cfcb-0a99-4149-bcee-8535113cc417",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train stratification distribution:\\n\", stratify_train.value_counts(normalize=True))\n",
    "print(\"Calib stratification distribution:\\n\", stratify_calib.value_counts(normalize=True))\n",
    "print(\"Test stratification distribution:\\n\", stratify_test.value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4b2806-49ac-4cb2-bda9-3a455fe33be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14c1cdf-5d18-4383-9e31-13b52bec18dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!!!Caution, siab not grouped yet for year\n",
    "label = siab[\"ltue\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478de97e-53f5-48bc-9830-612ec915c0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(siab[\"ltue\"].value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eea0cd4-1b51-467a-a649-d4605e8e8e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select stratification strategy\n",
    "if universe[\"stratify_split\"] == \"none\":\n",
    "    stratify = None\n",
    "elif universe[\"stratify_split\"] == \"target\":\n",
    "    stratify = label\n",
    "elif universe[\"stratify_split\"] == \"protected-attribute\":\n",
    "    stratify = siab[\"maxdeutsch1\"].astype(str) # ?????? Do I need maxdeutsch1.missing?\n",
    "elif universe[\"stratify_split\"] == \"both\":\n",
    "    # Concatinate both columns\n",
    "    stratify = siab[\"maxdeutsch1\"].astype(str) + \"-\" + label.astype(str) # ?????? Do I need maxdeutsch1.missing?\n",
    "    #stratify = features_org[\"maxdeutsch1\"].fillna(\"missing\").astype(str) + \"-\" + label.astype(str)\n",
    "\n",
    "\n",
    "stratify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838792d0-2920-4149-8848-cd3eb9cfe278",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Stratified train vs. temp split (80% train+calib / 20% test)\n",
    "\n",
    "X_temp, X_test, y_temp, y_test, strat_temp, strat_test = train_test_split( # ???? org_test, org_temp????\n",
    "    X,\n",
    "    y,\n",
    "    stratify_col,  # could be target, protected attribute, or combo\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=stratify_col\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e54499-23a1-45a9-bff8-8d62bce2bf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified train vs. calibration (from the temp set)\n",
    "\n",
    "X_train, X_calib, y_train, y_calib, strat_train, strat_calib = train_test_split(\n",
    "    X_temp,\n",
    "    y_temp,\n",
    "    strat_temp,\n",
    "    test_size=0.25,  # because 0.25 * 0.8 = 0.2 final calib\n",
    "    random_state=42,\n",
    "    stratify=strat_temp\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da0b463-bb4c-4f7a-b7e4-9de0a3db812e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "(\n",
    "    X_train, X_test,\n",
    "    y_train, y_true,\n",
    "    group_train, group_test,\n",
    "    org_train, org_test\n",
    ") = train_test_split(\n",
    "    features,\n",
    "    label,\n",
    "    group,\n",
    "    features_org,\n",
    "    test_size=0.2,\n",
    "    # Note: The analysis originally used two distinct seeds, one for numpy (defaulting to 2023) and one for the train_test_split (defaulting to 0).\n",
    "    # To allow for exact reproducibility of the original results, as well as specification of only a single seed we base this second seed off the first one.\n",
    "    # If you adapt this code for your own analysis feel free to remove this line and replace it e.g. with e.g. a call to numpy.random.randint.\n",
    "    random_state=abs(parsed_seed - 2023),\n",
    "    stratify=stratify\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c4dc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXCLUDE CERTAIN SUBGROUPS\n",
    "# ----------------------\n",
    "\n",
    "mode = universe.get(\"exclude_subgroups\", \"keep-all\") \n",
    "# Fetches the exclude_subgroups setting from the universe dict.\n",
    "# Defaults to \"keep-all\" if the key is missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21994072",
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode == \"keep-all\":\n",
    "    keep_mask = pd.Series(True, index=org_train.index)\n",
    "\n",
    "# org_train contains the original feature columns from features_org (in Simson)\n",
    "# features_org contains unprocessed features, for me X_train at beginning ???\n",
    "# For keep-all, creates a boolean Series (keep_mask) of all True, so no rows are removed.\n",
    "\n",
    "elif mode == \"drop-non-german\":\n",
    "    keep_mask = org_train[\"maxdeutsch1\"] == 1 # ??? what about missing values?\n",
    "\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported mode for exclude_subgroups: {mode}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71651440",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_drop = (~keep_mask).sum() # Calculates how many rows are set to be dropped\n",
    "if n_drop > 0:\n",
    "    pct = n_drop / len(keep_mask) * 100\n",
    "    print(f\"Dropping {n_drop} rows ({pct:.2f}%) where mode='{mode}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb8eee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[keep_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b202bb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train[keep_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493e2ac3",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679e1268",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "\n",
    "if (universe[\"model\"] == \"logreg\"):\n",
    "    model = LogisticRegression() #penalty=\"none\") #, solver=\"newton-cg\", max_iter=1) # include random_state=19 ?\n",
    "elif (universe[\"model\"] == \"penalized_logreg\"):\n",
    "    model = LogisticRegression(penalty=\"l2\", C=1.0) #, solver=\"newton-cg\", max_iter=1)\n",
    "elif (universe[\"model\"] == \"rf\"):\n",
    "    model = RandomForestClassifier(n_estimators=100, n_jobs=-1)\n",
    "elif (universe[\"model\"] == \"gbm\"):\n",
    "    model = GradientBoostingClassifier()\n",
    "elif (universe[\"model\"] == \"elasticnet\"):\n",
    "    model = LogisticRegression(penalty = 'elasticnet', solver = 'saga', l1_ratio = 0.5, max_iter=5000) # which solver to use?\n",
    "else:\n",
    "    raise \"Unsupported universe.model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc8f744",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "model = Pipeline([\n",
    "    #(\"continuous_processor\", continuous_processor),\n",
    "    #(\"categorical_preprocessor\", categorical_preprocessor),\n",
    "    (\"scale\", StandardScaler() if universe[\"scale\"] == \"scale\" else None), \n",
    "    (\"model\", model),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9442c04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af59f8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairness_multiverse.universe import predict_w_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cf2e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_test = model.predict_proba(X_test)\n",
    "\n",
    "'''\n",
    "Below code returns a boolean array (or binary 0/1 array depending on how it’s used) where each element \n",
    "is True if the probability of class 1 is greater than or equal to the threshold, and False otherwise.\n",
    "'''\n",
    "y_pred_default = predict_w_threshold(probs_test, 0.5)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Naive prediction\n",
    "accuracy_score(y_true = y_true, y_pred = y_pred_default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081964c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c9705b",
   "metadata": {},
   "source": [
    "# Conformal Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160ec6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Miscoverage level for conformal prediction (10% allowed error rate => 90% target coverage)\n",
    "alpha = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadf4555-3dd3-440e-8e35-f82a4ad9f855",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_calib = model.predict_proba(X_calib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed93547-6d5d-4983-9b36-1ecb300da49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_calib = y_calib.values.ravel().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d1320d-f588-4b38-9072-62af1ae97f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairness_multiverse.conformal import compute_nc_scores\n",
    "\n",
    "# Compute nonconformity scores on calibration set (1 - probability of true class)\n",
    "nc_scores = compute_nc_scores(probs_calib, y_calib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3b8ca3-53b7-43d5-9667-7c85da7aeda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairness_multiverse.conformal import find_threshold\n",
    "\n",
    "# Find conformal threshold q_hat for the given alpha (split conformal method)\n",
    "q_hat = find_threshold(nc_scores, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d29e6c1-0ef6-4aa4-b8fe-4fe79b0d033a",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92460794-cdac-4be2-ba28-f28c0515a6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairness_multiverse.conformal import predict_conformal_sets\n",
    "\n",
    "# Generate prediction sets for each test example\n",
    "pred_sets = predict_conformal_sets(model, X_test, q_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90c9a65-e6db-4f5d-80cd-e68fb7e46829",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = y_true.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a58a54a-1e68-46b9-927a-df01f18aebc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairness_multiverse.conformal import evaluate_sets\n",
    "\n",
    "# Evaluate coverage and average set size on test data\n",
    "metrics = evaluate_sets(pred_sets, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce0a5cb-201f-45a0-ade5-94c7f0bd6095",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9a838d-31c9-430b-ae14-12baee460d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_universe = universe.copy()\n",
    "universe_model = example_universe.get(\"model\")\n",
    "universe_exclude_features = example_universe.get(\"exclude_features\")\n",
    "universe_exclude_subgroups = example_universe.get(\"exclude_subgroups\")\n",
    "universe_scale = example_universe.get(\"scale\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b6a8ac-9dc0-4245-8a91-5b3b5999c4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_metrics_dict = {\n",
    "    \"universe_id\": [universe_id],\n",
    "    \"universe_model\": [universe_model],\n",
    "    \"universe_exclude_features\": [universe_exclude_features],\n",
    "    \"universe_exclude_subgroups\": [universe_exclude_subgroups],\n",
    "    \"universe_scale\": [universe_scale],\n",
    "    \"q_hat\": [q_hat],\n",
    "    \"coverage\": [metrics[\"coverage\"]],\n",
    "    \"avg_size\": [metrics[\"avg_size\"]],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908acbb1-0371-4915-85ca-3fa520d2efe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33007efc-14e9-4ec6-97ac-56a455c82265",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_metrics_df = pd.DataFrame(cp_metrics_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db12611b-57b2-4910-b9d0-355adfd6e7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fbde55-a0a1-47e4-9af6-fe4a17fa8c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conditional Coverage & looking at subgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027ae886-68dd-4141-96c7-805a5450e1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairness_multiverse.conformal import build_cp_groups\n",
    "\n",
    "cp_groups_df = build_cp_groups(pred_sets, y_true, X_test.index, org_test)\n",
    "#needs universe_id and setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d5802c-663a-4d1f-9d69-e036faaffbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete\n",
    "cp_groups_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa424af1-d1eb-443d-b717-2d279ada8408",
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete\n",
    "percentage = (cp_groups_df['nongerman_female'] == 1).mean() * 100\n",
    "print(percentage)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7d5bbd64-b81d-418d-9ef3-1c9e27eb89c1",
   "metadata": {},
   "source": [
    "# Define covered = 1 if true_label is in the predicted set\n",
    "cp_groups_df['covered'] = cp_groups_df.apply(\n",
    "    lambda r: int(r['true_label'] in r['pred_set']),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338ff529-1a6b-4b87-a931-8e4788d52aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cp_groups_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90032e27-66df-4540-9169-4e89cfbaa758",
   "metadata": {},
   "outputs": [],
   "source": [
    "subgroups = ['frau1','nongerman','nongerman_male','nongerman_female']\n",
    "\n",
    "# Conditional coverage for subgroup==1\n",
    "cond_coverage = {\n",
    "    g: cp_groups_df.loc[cp_groups_df[g]==1, 'covered'].mean()\n",
    "    for g in subgroups\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e348fb1-b9c3-45bc-ba23-f016827a7e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5093324-c3dd-4d63-bc4c-8d945e5c8bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for subgroup, cov in cond_coverage.items():\n",
    "    cp_metrics_df[f\"cov_{subgroup}\"] = cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e872e10-dbe5-49e8-b554-5030c98ff75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33774451",
   "metadata": {},
   "source": [
    "# (Fairness) Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debc5d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do I need to include maxdeutsch1.missing?\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "colname_to_bin = \"maxdeutsch1\"\n",
    "majority_value = org_train[colname_to_bin].mode()[0]\n",
    "\n",
    "org_test[\"majmin\"] = np.where(org_test[colname_to_bin] == majority_value, \"majority\", \"minority\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a50de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_universe = universe.copy()\n",
    "example_universe[\"cutoff\"] = example_universe[\"cutoff\"][0]\n",
    "example_universe[\"eval_fairness_grouping\"] = example_universe[\"eval_fairness_grouping\"][0]\n",
    "fairness_dict, metric_frame = universe_analysis.compute_metrics(\n",
    "    example_universe,\n",
    "    y_pred_prob=probs_test,\n",
    "    y_test=y_true,\n",
    "    org_test=org_test,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95d4e02",
   "metadata": {},
   "source": [
    "# Overall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdee4871",
   "metadata": {},
   "source": [
    "Fairness\n",
    "Main fairness target: Equalized Odds. Seems to be a better fit than equal opportunity, since we're not only interested in Y = 1. Seems to be a better fit than demographic parity, since we also care about accuracy, not just equal distribution of preds.\n",
    "\n",
    "Pick column for computation of fairness metrics\n",
    "\n",
    "Performance\n",
    "Overall performance measures, most interesting in relation to the measures split by group below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e067c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_frame.overall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e968fe9d",
   "metadata": {},
   "source": [
    "By Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec325bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_frame.by_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ddbe63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In a graphic\n",
    "metric_frame.by_group.plot.bar(\n",
    "    subplots=True,\n",
    "    layout=[3, 3],\n",
    "    legend=False,\n",
    "    figsize=[12, 8],\n",
    "    title=\"Show all metrics\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57a0bac",
   "metadata": {},
   "source": [
    "# Final Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d280a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_universes = universe_analysis.generate_sub_universes()\n",
    "len(sub_universes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92adf7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_sub_universe_data(sub_universe, org_test):\n",
    "    # Keep all rows — no filtering\n",
    "    keep_rows_mask = np.ones(org_test.shape[0], dtype=bool)\n",
    "\n",
    "    print(f\"[INFO] Keeping all rows: {keep_rows_mask.sum()} rows retained.\")\n",
    "    return keep_rows_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a759155",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output = universe_analysis.generate_final_output(\n",
    "    y_pred_prob=probs_test,\n",
    "    y_test=y_true,\n",
    "    org_test=org_test,\n",
    "    filter_data=filter_sub_universe_data,\n",
    "    cp_metrics_df=cp_metrics_df,\n",
    "    save=True,\n",
    ")\n",
    "final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721ff514-cb11-46f8-b6fc-50392f74b5cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python (CMA Fairness)",
   "language": "python",
   "name": "cma_fair_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
